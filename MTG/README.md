Documentation  {#mainpage}
=============

Introduction {#introduction}
============

These scripts were developed between 2012-2014 by Max Graves while obtaining
an M.S. Materials Science degree from UVM.  The purpose of this readme file
is to document the purpose, usage, and design decisions of the scripts
located in this /MTG directory.  I (MTG) make no guarantee that these are all
still fully functional, as many of them were used only very briefly then 
evolved into other scripts often for different but similar purposes.  My
style of writing Python also evolved, so I ask users to forgive the lack of 
proper style in some of the earlier scripts.

This is in no way an exhaustive documentation.  Most scripts use the argparse
module to take command line options, so running 'scriptName.py --help' from
terminal will give you most of the script options.  Some scripts also have
subparsers.  I took care to document the usage of these where applicable.
Many of the scripts are also well commented so usage and purpose should
be clear.

NOTES
-----
- Some of these were adapted from scripts in the SCRIPTS top-level
directory and carry the same name.  Changes are (mostly) noted here.
- The assumption is made that users are working from a terminal.
- Scripts are documented in alphabetical order.
- Many of these require installation of non-standard Python modules.  All of
these are available in standard repositories such as Aptitude or Macports
or can be obtained with a utility such as pip.


bipartitionPOVplot.py
=====================

Script for generation spatial and particle bipartition images used in the 
entanglement entropy paper written by Chris Herdman and Adrian Del Maestro.
Also can create planar bipartitioning images.  It starts with a cubic
lattice and randomly displaces the atoms to look like they are in the 
continuum.  This uses the method of creating POV-ray images from Visual
Python scenes.

Usage
-----
Issuing:
    python bipartitionPOVplot.py
will generate an image named 'bipartView.png' and a file named 
'bipartView.pov' in the current working directory.

Notes
-----
The type of partitioning along with other options such as
the number of particles (lattice size) are set at the top.


bipartPOVexport.py
==================

A modification of povexport.py that has the preferred finish characteristics
set for the images generated by bipartitionPOVplot.py.  Nothing else is
different from povexport.py.

See povexport.py documentation in this file for comments.


clusterTools.py
===============

Module containing functions to allow automated job submission and data
retrieval to/from the VACC.

Usage
-----
import into a script to use functions.  Examples exist in:
- pushSeedDirs.py
- pullSeedDirs.py
- distributeSeedDirs.py
- restartSeedDirs.py


MTGplot_scalar_estimator.py
============================
MTGplot_tensor_estim.py
========================

MTGpimcplot.py
===============

Very similar to the pimcplot.py script in the top-level SCRIPTS directory.
Differences include added cmd line option to save pdfs, changed from 
--nobin to --bin to not automatically generate histograms, added latex
fonts, an autocorrelation function added, and various other small additions
that I saw fit to use over time.


plotWorldLinesMPL.py
====================
seedDirDistribute.py
====================
QHOspecificHeat.py
===================

pimchelp.py 
===========

Module that holds some functions for analyzing data.

This is exactly the same as the one in the SCRIPTS top-level directory.  It 
was moved here to not deal with changing the path on scripts already written
when importing it.


QHOxSquared.py
===============
seedDirRestart.py
=================
detTCvaryingNumPart.py
=======================
plotClassicalState.py
=====================
jackKnife.py  
============
plotEnergyVSvirialWindow.py 
===========================
scaleEstims_multHeaders_S.py
============================
jkTools.py
==========
plotJKdata_S.py   
===============
scaleEstims_singHeaders1_S.py
=============================
localPermVideo.py 
=================
plotOBDM.py
===========
scaleEstims_singHeaders2_S.py
=============================
MTG_CH_gensubmit.py
===================
plotWinding_bulkScale.py
========================
scaleEstims_Vfilm_S.py
======================

povexport.py
============

This serves as a library of functions to convert a visual Python scene
into a POV-ray image.  I did not write this, only modified it to meet my 
own needs.

Usage
-----
import into a Python script and call export after creating a VPython
scene.  An example of this is done in vispath.py, which also uses
visTools.py to visualize PIMC worldlines. 

Notes
-----
- You can change finishes of objects, colors, backgrounds, camera angles, and
other such options as is done after the obnoxious header comments.
- VPython cannot handle transparent curves, so I circumvented this by defining
cylinderTrans where all finishe characteristics are defined.
- A user wanting to use this should spend some time investigating how it works,
as I am not even close to fully documenting it here.


visTools.py
===========

This contains all of the functions to visualize the PIMC worldlines in Visual
Python in either 1d, 2d, or 3d.  It also contains some functions that go
into converting these VPython scenes into POV-ray images, including a function
to write the .ini files for animated POV-ray scenes.

Usage
-----
import into a Python script (ideally along with povexport to produce beautiful
POV-ray images) and call its functions.  An example of this is done in 
vispath.py.


vispath.py 
==========

Main script for visualizing PIMC worldlines in visual Python and POV-ray.

Usage
-----
python vispath.py /path/to/gce-wl-...

Notes
-----
- This allows for single frame visualization, rotating POV-ray scene (via
creation of .ini file) currently.
- The command line option '-o bins' is not currently implemented, but its
purpose is to visualize the Monte Carlo time evolution of the worldlines,
and should be trivially implemented.



seedDirPush.py
==============

Script for submitting large numbers of jobs to the VACC without having
to directly ssh into it.  In running our PIMC jobs, it is possible to run many
jobs with the same set of parameters and different random number seeds,
then put all of the data together and consider it uncorrelated.  This can
save an enourmous amount of time, as it is truly parallel.

Usage
-----
Example:
If I want to submit for a range of temperatures, I create a directory on my
local machine called /exampleRun/.  I generally would then create the
subdirectory /exampleRun/submitStuff/ which contains a file called 'submit'.
The sole contents of this file will look something like:
     -N 32 -u -7.2 -n 0.02198 -t 0.01 -M 20 -C 1.0 -I aziz -X free -E 10000 -S 1000 -V 5 -l 7 --relax
     T 1.0:5.0:1.0
From within this /exampleRun/submitStuff/ directory, I will then issue:
     python pushSeedDirs.py -t './target/path/on/cluster/' push -L 0 -H 1
- This will ask for your username and password
- This builds /target/path/on/cluster/seed000 and 
/target/path/on/cluster/seed001 directories into your home directory on the 
VACC, each containing directories out and OUTPUT, and a pbs torque submit
script that has been generated from MTG_CH_gensubmit.py.
- If the '-s' option is supplied at the time of running the script, the jobs
set up will be automatically submitted for you.

Notes
-----
- See comments in script, mainly those starting with 'NOTE:'.
- User needs to change some options near the top of the script related
to their username, etc..
- If you do not include the './' when using the -t flag, it will not work.
- BE COURTEOUS WHEN USING THIS SCRIPT!!
- python pushSeedDirs.py -t './target/path/on/cluster' push --help
will give you command line options...'push' is a subparser option.


seedDirPull.py
==============

Script for pulling data back from cluster that have been submitted by
pushSeedDirs.py (or restartSeedDirs, distributeSeedDirs).  This script will
retrieve all data files in all of the seedXXX files within a given 
directory on the cluster, and create two new types of data files, using the
'crunchData' function in clusterTools.py.  The first type of data file
is called a Reduced data file, which contains all of the data from all files,
neatly organized in CSV formatted columns.  The second type of file created
is called zAveraged files (the z is added to put it at the end of the list
then 'ls' is issued..hacky I know) which contains the average, standard error,
and number of bins for each individual data file.  Numerous scripts exist
for the evaluation of these data files.  These include 

Usage
-----
Issuing:
    python /path/to/pullSeedDirs.py -t './target/path/on/cluster' pull -c
from within the directory you want to pull all of the data into will pull down
all of the data files from all seedXXX directories, changing the first three 
numbers of the pimcID to XXX (the seed number padded with 0's) in order to
avoid non-unique data file names.  The '-c' flag tells it to create the Reduced
and zAveraged files.

Notes
-----
- At the top, lists are created with the types of data files to pull, the 
column numbers they are in in the gce-... file (starting with 0), and the name
of the estimator.  These must be ordered.



plot

