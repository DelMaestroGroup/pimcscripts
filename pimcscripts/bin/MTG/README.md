Documentation  {#mainpage}
=============

Author:         Max Graves
Last Modified:  16-APR-2014


Introduction {#introduction}
============

These scripts were developed between 2012-2014 by Max Graves while obtaining
an M.S. Materials Science degree from UVM.  The purpose of this readme file
is to document the purpose, usage, and design decisions of the scripts
located in this /MTG directory.  I (MTG) make no guarantee that these are all
still fully functional, as many of them were used only very briefly then 
evolved into other scripts often for different but similar purposes.  My
style of writing Python also evolved, so I ask users to forgive the lack of 
proper style in some of the earlier scripts.

This is in no way an exhaustive documentation.  Most scripts use the argparse
module to take command line options, so running 'scriptName.py --help' from
terminal will give you most of the script options.  Some scripts also have
subparsers.  I took care to document the usage of these where applicable.
Many of the scripts are also well commented so usage and purpose should
be clear.

NOTES
-----
- Some of these were adapted from scripts in the SCRIPTS top-level
directory and carry the same name.  Changes are (mostly) noted here.
- The assumption is made that users are working from a terminal.
- Scripts are documented in alphabetical order.
- Many of these require installation of non-standard Python modules.  All of
these are available in standard repositories such as Aptitude or Macports
or can be obtained with a utility such as pip.
- All underscores in filenames have been changed to dashes in this readme
file because of annoying .md syntax highlighting.


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
bipartitionPOVplot.py
=====================

Script for generation spatial and particle bipartition images used in the 
entanglement entropy paper written by Chris Herdman and Adrian Del Maestro.
Also can create planar bipartitioning images.  It starts with a cubic
lattice and randomly displaces the atoms to look like they are in the 
continuum.  This uses the method of creating POV-ray images from Visual
Python scenes.

Usage
-----
Issuing:
    python bipartitionPOVplot.py
will generate an image named 'bipartView.png' and a file named 
'bipartView.pov' in the current working directory.

Notes
-----
- The type of partitioning along with other options such as the number of 
particles (lattice size) are set at the top.
- Need rgb tuples to be on the same order.  Making one set of values much 
larger than another seriously messes with the color contrasts.
- You may need to vary the finish characteristics inside of bipartPOVexport.py
to get the exact look that you want.
- Need to move the finish setting from bipartPOVexport.py into this script!


-------------------------------------------------------------------------------
bipartPOVexport.py
==================

A modification of povexport.py that has the preferred finish characteristics
set for the images generated by bipartitionPOVplot.py.  Nothing else is
different from povexport.py.

See povexport.py documentation in this file for usage.

Notes
-----
- If you want to get colors exactly right, you need to tinker with finishes
as are set up near the top of the script (after long comment section).  These
along with scaling the rgb values inside of bipartitionPOVplot.py can give you
a wide range of brilliance, robustness of color, etc...


-------------------------------------------------------------------------------
clusterTools.py
===============

Module containing functions to allow automated job submission and data
retrieval to/from the VACC.

Usage
-----
import into a script to use functions.  Examples exist in:
- pushSeedDirs.py
- pullSeedDirs.py
- distributeSeedDirs.py
- restartSeedDirs.py


-------------------------------------------------------------------------------
detTCvaryingNumPart.py
======================

Script that plots superfluid fraction for systems with varying numbers of 
particles that have been studied with pimc and arranged in a directory
structure as described in the usage.

Usage
-----
Issue:
    python detTCvaryingNumPart.py 
within a directory containing varying directories named 'Nxxx', where 'xxx' is
to denote an integer number of particles.  Each 'Nxxx' directory will then
contain its own directory named 'OUTPUT' that contains gce-super-... files
to be reduced using reduce-one.py in the top-level SCRIPTS directory.  It then
takes the reduced scripts and performs polynomial fitting to try to fit the
data and determine the transition temperature.

Notes
-----
- This may need lots of modification, as it hasn't been used in a long time.  I
included it here because it is an example of how one may call the reduce-one.py
script from within a script.
- Transition temperatures are found through looking at the specific heat, and
fitting of this has been shown to be somewhat effective using spline fitting
and polynomial fitting.
- I did not test this at the time of this writing.


-------------------------------------------------------------------------------
jackKnife.py  
============

Script for producing files with jackknife averages and standard errors from 
a directory full of gce-... files with varying random number seeds.

Usage
-----
Issue:
    python jackKnife.py ReducedfName.dat -options
will read in ReducedfName.dat file(s) and generate Jackknife data files
containing the jackknife averages as well as standard errors.  It will
then plot the resulting estimators versus temperature.

Notes
-----
- This works for Reduced... files containing ranges of temperatures.  Types
of estimators currently implemented are energy, specific heat, film/bulk
densities, superfluid fraction, angular winding, and standard topological
winding numbers.
- Possibility exists that the standard error could be incorrectly produced 
here, as it gives smaller errorbars than would be expected sometimes.
- This has not been used in awhile, and was not tested at the time of writing
this file.  Therefore, modifications should be made before using.
- This was given up on in favor of using the scaleEstims... scripts that 
compute the averages and standard errors from the zAveraged... data files.


-------------------------------------------------------------------------------
jkTools.py
==========

Header file containing a function with the jackknife method as well as a
function that is called for command line parsing for jackKnife.py and
scaleEstims-singHeader2.py.

Usage
-----
Example seen in jackKnife.py

Notes
-----
- Outline of jackknife method is given in the M.S. thesis by Max Graves.


-------------------------------------------------------------------------------
localPermVideo.py 
=================

Produces a series of images generated from gce-locperm... data files that are
then converted to a video format.  This shows the evolution of the local
permutation number (see thesis by Max Graves) with increasing temperature.
This quantity can be thought of as a qualitative way of picturing local
superfluidity in a quantum fluid system.

Usage
-----
Issue:
    python localPermVideo.py
within a directory full of gce-locperm... data files, each from a run at a
distinct temperature.  This will generate .png images which are then
converted to either an .avi or .mpg file (see bottom of script).

Notes
-----
- This required the utility mencoder to convert bitmap images to a video
format.  The script actually checks for this utility and will let you know
if you don't have it installed on your machine.


-------------------------------------------------------------------------------
MTG-CH-gensubmit.py
===================

This is very similar to gensubmit.py in the top-level SCRIPTS directory.  It 
generates torque submission scripts for various clusters.  This copy is
an amalgamation of the original with my own modification and a version 
inherited by myself (Max Graves) from Chris Herdman (hence the MTG-CH-).
This is most useful when generating array jobs or checkpointed jobs.

It is set up to work for various clusters, and the only thing that should be 
different between this one the the original is that bluemoon has been modified
and bluemoon-blcr has been added.  The latter is set up for Berkeley lab
checkpointing.  Other differences may include the ability to set up jobs
for pigs code, 1d job submission, extra memory request for cluster.

This will submit jobs to the temp directories directly on the nodes for VACC 
submissions.  For jobs that write a lot of data to disk frequently, this is a 
much faster way to operate.  Once jobs are finished, the data is zipped up and
copied back to the directory they were submitted from on the cluster.

Usage
-----
Issue:
    python MTG-CH-gensubmit.py submit --options
where dashes are actually underscores.  The file 'submit' should look like the
on described in seedDirsPush.py in this file.  This will generate a submit
file in your current working directory that can be copied over to the cluster
and submitted using standard torque commands.

Notes
-----
- This gets called by seedDirsPush.py using the Python module subprocess.
- For the --pigs option, an executable named 'pigs' must be on your path on
the cluster you are submitting to.
- For the --oneD option, an executable named 'pimc1D' must be on your path on
the cluster you are submitting to.
- It is advised that if you are using this for the first time that you 
read the VACC (or whatever cluster you are using) documentation to become
familiar with more basic submit files than these.


-------------------------------------------------------------------------------
MTGpimcplot.py
===============

Very similar to the pimcplot.py script in the top-level SCRIPTS directory.
Differences include added cmd line option to save pdfs, changed from 
--nobin to --bin to not automatically generate histograms, added latex
fonts, an autocorrelation function added, and various other small additions
that I saw fit to use over time.


-------------------------------------------------------------------------------
MTGplot-scalar-estimator.py
============================

Very similar to plot-scalar-estimator.py script in the top-level SCRIPTS
directory.  Difference is mainly in ability to plot the same type of estimator
multiple times on the same plot for comparison.


-------------------------------------------------------------------------------
MTGplot-tensor-estimator.py
========================

Very similar to plot-tensor-estimator.py script in top-level SCRIPTS directory.
Differences include addition of latex fonts for plotting and various small
tweaks that were favorable for plotting excluded volume spatial density
projections.


-------------------------------------------------------------------------------
pimchelp.py 
===========

Module that holds some functions for analyzing data, etc...

This is exactly the same as the one in the SCRIPTS top-level directory.  It 
was moved here to not deal with changing the path on scripts already written
when importing it.


-------------------------------------------------------------------------------
plotClassicalState.py
=====================

Used to visualize the initial seeding of particles inside of an excluded 
volume simulation cell.  This was only used when testing how to place 
particles inside of a cell with excluded volume in their classical state,
i.e. when all of the beads of a worldline are at the same location.  Therefore
it will not draw links or handle quantum states.

Usage
-----
Issue:
    python plotClassicalState.py initialConfig.dat
where initialConfig.dat is a data file that is generated from the pimc code
whenever the excluded volume potential is first seeded with classical atoms.

Notes
-----
- Uses matplotlib 3d plotting to visualize the cell and atom positions.
- If you want to visualize full worldlines and have nicer images, look to 
the vispath.py and vistools.py scripts.


-------------------------------------------------------------------------------
plotEnergyVSvirialWindow.py 
===========================

Used in scaling of virial energy errorbars as a function of the virial window
size.  For explanation of what these are, see the thesis by Max Graves.

Usage should be obvious from looking at script.


-------------------------------------------------------------------------------
plotJKdata-S.py 
===============

Used for plotting data from Jackknife... data files for various values of 
bulk separation (from excluded volume project).

Usage
-----
Issue:
    python plotJKdata-S.py --options
where the dash is an underscore, from within a directory containing
directories labelled alphaS where S is an integer between 1-9.  Each of these 
subdirectories should have Jackknife... data files in them corresponding 
to each bulk separation value.

Notes
-----
- This needs to be fixed to deal with bulk separations larger than 9 ang.
- This was abandoned for the scaleEstim... analysis scripts, but was left in 
this repository in case someone decided to use jackknife analysis in a similar 
way so that they could have something to build on.
- The usage of 'alpha' instead of 'S' was an artifact of how we originally 
denoted the bulk separation.


-------------------------------------------------------------------------------
plotOBDM.py
===========

Script to read in OBDM files from code and compute Fourier Transform in order
to get back the BEC fraction.  This loops over a range of temps (data files)
and plots condensate fraction for each temperature.

Usage
-----
Issue:
    python plotOBDM.py
in a directory containing a subdirectory named 'data' that contains gce-obdm..
data files for a varying range of temperatures.

Notes
-----
- This pulls temperature from data file name, so this script will need to be 
modified if ce-... files will be analyzed.


-------------------------------------------------------------------------------
plotWinding-bulkScale.py
========================

ADD THIS!!!!

Usage
-----
ADD THIS!!

Notes
-----
ADD THIS!!!


-------------------------------------------------------------------------------
povexport.py
============

This serves as a library of functions to convert a visual Python scene
into a POV-ray image.  I did not write this, only modified it to meet my 
own needs.

Usage
-----
import into a Python script and call export after creating a VPython
scene.  An example of this is done in vispath.py, which also uses
visTools.py to visualize PIMC worldlines. 

Notes
-----
- You can change finishes of objects, colors, backgrounds, camera angles, and
other such options as is done after the obnoxious header comments.
- VPython cannot handle transparent curves, so I circumvented this by defining
cylinderTrans where all finishe characteristics are defined.
- A user wanting to use this should spend some time investigating how it works,
as I am not even close to fully documenting it here.


-------------------------------------------------------------------------------
QHOspecificHeat.py
==================

Creates a Jackknife... data file for the 1d quantum harmonic oscillator
specific heat and energy data.  This was used in preliminary testing of the 
specific heat estimator based on the centroid virial energy estimator.  Also
plots the data from the jackknife analysis, plotting energy and specific heat.

Usage
-----
issue:
    python QHOspecificHeat.py FileNames
where FileNames are the gce-estimator... files generated from the pimc code.

Notes
-----
- Really slow for large data files.  Use the '-s' flag to skip some number of 
data points from the first part of the file... '-s 5' skips 5 data points.


-------------------------------------------------------------------------------
scaleEstims-multHeaders-S.py
============================

ADD THIS!!!!
Should be almost identical to below.


-------------------------------------------------------------------------------
scaleEstims-singHeaders1-S.py
=============================

ADD THIS!!!!
Needs to be combined with scaleEstims-singHeaders2-S.py and then documented.


-------------------------------------------------------------------------------
scaleEstims-singHeaders2-S.py
=============================

Needs to be deleted after combination with above.


-------------------------------------------------------------------------------
scaleEstims-Vfilm-S.py
======================

Plots excluded volume data including superfluid fraction, film density, bulk 
density, number of particles in film, angular winding, all versus the 
film potential for various values of bulk separation.  This was used to 
perform the scaling of the film lowering potential energy to find correct
film densities in the excluded volume cell for each bulk separation.

Usage
-----
Issue:
    python scaleEstims-Vfilm-S.py
from within a directory containing directories
    top/{T*}/{S*}/{V*}/{zAveraged*}.dat
where the curly brackets denote sets of directories of the form inside of them
and the * are temperatures, bulk separations, potential changes in film region,
and the various types of zAveraged... data files generated by seedDirPull.py.

Notes
-----
- You must supply information about the cell at the top of the main() function
so read through the script before using it.


-------------------------------------------------------------------------------
seedDirDistribute.py
====================

Script to be used when one single gce-state... file has been equilibrated for 
some long period of time and then multiple jobs will be spawned from this one 
state, but all given different random number seeds and equilibrated some more 
before data starts to be collected.

Usage
-----
ADD THIS!!!!

Notes
-----
- This is set up to work for a single set of job parameters and requires that
your home machine as well as VACC account have a directory structure set up
that contains a top level directory with subdirectories named:  'stateFiles'
and 'logFiles', each containing the gce-state... or gce-log... file from 
the equilibrated job.  The double storage is so that the user must really
think about what they are doing.


-------------------------------------------------------------------------------
seedDirPull.py
==============

Script for pulling data back from cluster that have been submitted by
pushSeedDirs.py (or restartSeedDirs, distributeSeedDirs).  This script will
retrieve all data files in all of the seedXXX files within a given 
directory on the cluster, and create two new types of data files, using the
'crunchData' function in clusterTools.py.  The first type of data file
is called a Reduced data file, which contains all of the data from all files,
neatly organized in CSV formatted columns.  The second type of file created
is called zAveraged files (the z is added to put it at the end of the list
then 'ls' is issued..hacky I know) which contains the average, standard error,
and number of bins for each individual data file.  Numerous scripts exist
for the evaluation of these data files.  These include 

Usage
-----
Issuing:
    python /path/to/pullSeedDirs.py -t './target/path/on/cluster' pull -c
from within the directory you want to pull all of the data into will pull down
all of the data files from all seedXXX directories, changing the first three 
numbers of the pimcID to XXX (the seed number padded with 0's) in order to
avoid non-unique data file names.  The '-c' flag tells it to create the Reduced
and zAveraged files.

Notes
-----
- At the top, lists are created with the types of data files to pull, the 
column numbers they are in in the gce-... file (starting with 0), and the name
of the estimator.  These must be ordered.


-------------------------------------------------------------------------------
seedDirPush.py
==============

Script for submitting large numbers of jobs to the VACC without having
to directly ssh into it.  In running our PIMC jobs, it is possible to run many
jobs with the same set of parameters and different random number seeds,
then put all of the data together and consider it uncorrelated.  This can
save an enourmous amount of time, as it is truly parallel.

Usage
-----
Example:
If I want to submit for a range of temperatures, I create a directory on my
local machine called /exampleRun/.  I generally would then create the
subdirectory /exampleRun/submitStuff/ which contains a file called 'submit'.
The sole contents of this file will look something like:
     -N 32 -u -7.2 -n 0.02198 -t 0.01 -M 20 -C 1.0 -I aziz -X free -E 10000 -S 1000 -V 5 -l 7 --relax
     T 1.0:5.0:1.0
From within this /exampleRun/submitStuff/ directory, I will then issue:
     python pushSeedDirs.py -t './target/path/on/cluster/' push -L 0 -H 1
- This will ask for your username and password
- This builds /target/path/on/cluster/seed000 and 
/target/path/on/cluster/seed001 directories into your home directory on the 
VACC, each containing directories out and OUTPUT, and a pbs torque submit
script that has been generated from MTG-CH-gensubmit.py.
- If the '-s' option is supplied at the time of running the script, the jobs
set up will be automatically submitted for you.

Notes
-----
- See comments in script, mainly those starting with 'NOTE:'.
- User needs to change some options near the top of the script related
to their username, etc..
- If you do not include the './' when using the -t flag, it will not work.
- BE COURTEOUS WHEN USING THIS SCRIPT!!
- python pushSeedDirs.py -t './target/path/on/cluster' push --help
will give you command line options...'push' is a subparser option.


-------------------------------------------------------------------------------
seedDirRestart.py
=================

This script will restart jobs on the cluster using the internal checkpointing
of the production pimc code.  Specifically, it restarts jobs which have been 
dispatched for equivalent parameters and varying random number seeds, as would 
have been submitted by seedDirPush.py.

Usage
-----
issue:
    python seedDirRestart.py -t './path/to/jobs/' push -L lSeed -H hSeed -s
where /path/to/jobs is the directory containing the seedXXX directories that
you want to restart.

Notes
-----
- This does not work unless you have unzipped all data files that reside in
the seedXXX/OUTPUT/ directories.
- You can modify the number of bins at the top of this script.


-------------------------------------------------------------------------------
vispath.py 
==========

Main script for visualizing PIMC worldlines in visual Python and POV-ray.

Usage
-----
python vispath.py /path/to/gce-wl-...

Notes
-----
- This allows for single frame visualization, rotating POV-ray scene (via
creation of .ini file) currently.
- The command line option '-o bins' is not currently implemented, but its
purpose is to visualize the Monte Carlo time evolution of the worldlines,
and should be trivially implemented.


-------------------------------------------------------------------------------
visTools.py
===========

This contains all of the functions to visualize the PIMC worldlines in Visual
Python in either 1d, 2d, or 3d.  It also contains some functions that go
into converting these VPython scenes into POV-ray images, including a function
to write the .ini files for animated POV-ray scenes.

Usage
-----
import into a Python script (ideally along with povexport to produce beautiful
POV-ray images) and call its functions.  An example of this is done in 
vispath.py.




END README
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
