#!/usr/bin/python

import getpass
import math
import os
import subprocess
import sys
import time

# Function that gives direct access to bash shell
def sh(cmd):
    return subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).communicate()[0]

# Make working directory based on run id. Run id is current time. 
run_id = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
work_dir = "/local-scratch/%s/runs/%s" %(getpass.getuser(), run_id)
print "Work directory for this run is: %s" %(work_dir)
os.makedirs(work_dir)

# Copy necessary scripts/executables to work_dir
sh("cp remote-job-wrapper local-post-job pimc.e %s/" %(work_dir))

# Make a list of the state files to spawn from and copy them to work_dir
statelist = []
for thing in os.listdir(os.getcwd()):
    if os.path.isfile(thing):
        if 'gce-state' in thing: 
            statelist.append(thing)
            sh("cp %s %s/" % (thing, work_dir))

# Move into work_dir and open master DAG file
os.chdir(work_dir)
dagfd = open("master.dag", "w")

count = 0 
# Loop through every state file          
for origstatefile in statelist:
    # Clean up and grab simulation params from state file name. Will be used 
    # to create job id and spawn simulation below
    statefile=origstatefile.rstrip('.dat')
    params=statefile.split('-')
    params=filter(None, params)
    if '+' not in params[-3]:
        params[-3]='-'+params[-3]
    for i in range(len(params)-1):
        params[i] = params[i].strip('0')
        if params[i][-1] == '.':
            params[i] = params[i].replace('.','.0')
    params.pop(0)
    params.pop(0)
    
    # Loop through each process number and create a submit file for every process
    # number for each state file
    for p in range(1,3):
        count += 1
        
        # Create a unique job id used to name the directory for a particular job.
        # This directory is used to store .dat files from the OUTPUT directory 
        # generated by the code and to stage the state file
        job_id = 'T%s__L%s__u%s__t%s__p%d' % (params[0],params[1],params[2],params[3],p)
        # Make job dir and create path to state file
        os.makedirs(job_id)
        sh("cp %s %s"%(origstatefile,job_id))
        statepath=job_id+'/'+origstatefile
        
        # Command line args for executable. Pulled from state file name.
         
        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        # NOTE: ALL STATE FILES MUST BE FOR THE SAME RADIUS AND THE RADIUS 
        # IS HARDCODED IN 
        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        args = "-W 22 -T %.1f -n 0.0528 -r 9 -b cylinder -L %.0f -t 0.004 -I aziz -X hex_tube -l 7 -u %.1f -M 8 -S 1000 -E 40000 -p %d --action primitive --relax -s %s" % (float(params[0]),float(params[1]),float(params[2]),p,statepath)
    
        # create the job submit file
        subfd = open("%s.submit" %(job_id), "w")
        subfd.write("universe = vanilla\n")
        subfd.write("requirements = (isUndefined(TARGET.GLIDEIN_ResourceName) || ((MY.MachineAttrGLIDEIN_ResourceName1 =!= TARGET.GLIDEIN_ResourceName) && (MY.MachineAttrGLIDEIN_ResourceName2 =!= TARGET.GLIDEIN_ResourceName) && (MY.MachineAttrGLIDEIN_ResourceName3 =!= TARGET.GLIDEIN_ResourceName) && (MY.MachineAttrGLIDEIN_ResourceName4 =!= TARGET.GLIDEIN_ResourceName) && (MY.MachineAttrGLIDEIN_ResourceName5 =!= TARGET.GLIDEIN_ResourceName)))\n")
        subfd.write("periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > (60*60))\n")
        subfd.write("executable = remote-job-wrapper\n")
        subfd.write("arguments = %s %s\n" %(job_id, args))
        subfd.write("transfer_input_files = pimc.e, %s\n" %(job_id))
        subfd.write("transfer_output_files = %s\n" %(job_id))
        subfd.write("output = %s.out\n" %(job_id))
        subfd.write("error = %s.err\n" %(job_id))
        subfd.write("log = job.log\n")
        subfd.write("ShouldTransferFiles = YES\n")
        subfd.write("when_to_transfer_output = ON_EXIT\n")
        subfd.write("+projectname = \"TG-DMR140072\"\n")
        subfd.write("notification = Never\n")
        subfd.write("queue 1\n")
        subfd.close()
   	    
        # update the dag
        dagfd.write("JOB          job_%d %s.submit\n" %(count, job_id))
        dagfd.write("SCRIPT POST  job_%d %s/local-post-job %s %s\n" %(count, work_dir, work_dir, job_id))
        dagfd.write("RETRY        job_%d 20\n" %(count))
        dagfd.write("\n")
        

print "%d jobs added. Now submitting DAG..." %(count)
dagfd.close()
sh("condor_submit_dag -notification Always -maxidle 500 master.dag")


